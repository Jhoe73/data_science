{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNodimp/bfWycj0oy8GV86Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhoe73/data_science/blob/main/CNN_accuracy_test2_db_fashion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyNtyuz3WALN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import random\n",
        "from tensorflow import keras\n",
        "from keras.datasets import fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# carregando datasets do keras\n",
        "#from tensorflow.keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "ApCDWYu_WEI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtendo informações das imagens (resolucao) e dos rótulos (número de classes)\n",
        "img_lin, img_col = x_train.shape[1], x_train.shape[2]\n",
        "num_classes = len(np.unique(y_train))\n",
        "print(x_train.shape)\n",
        "print('Classes: ', num_classes)"
      ],
      "metadata": {
        "id": "mwKSbHIIWFlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dividir por 255 para obter normalizacao\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "print(\"Antes da transformação: \", y_train[0])\n",
        "# transformar categorias em one-hot-encoding\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(\"Após transformação: \", y_train[0])"
      ],
      "metadata": {
        "id": "pnludkjSWHkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verifica imagens da base de dados tem 3 canais (RGB) ou apenas 1 (escala de cinza)\n",
        "if (len(x_train.shape) == 3):\n",
        "      n_channels = 1\n",
        "else:\n",
        "      n_channels = x_train.shape[3]\n",
        "\n",
        "# re-formata o array de forma a encontrar o formato da entrada (input_shape)\n",
        "# se a dimensão dos canais vem primeiro ou após a imagem\n",
        "if keras.backend.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], n_channels, img_lin, img_col)\n",
        "    x_test = x_test.reshape(x_test.shape[0], n_channels, img_lin, img_col)\n",
        "    input_shape = (n_channels, img_lin, img_col)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_lin, img_col, n_channels)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_lin, img_col, n_channels)\n",
        "    input_shape = (img_lin, img_col, n_channels)\n",
        "\n",
        "print(\"Shape: \", input_shape)"
      ],
      "metadata": {
        "id": "cRAxUIgTWJC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_cnn():\n",
        "    CNN = keras.Sequential()\n",
        "    CNN.add(keras.layers.Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=input_shape))\n",
        "    CNN.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "    CNN.add(keras.layers.Conv2D(64, kernel_size=(3,3), strides=(2,2), padding='same', activation='relu'))\n",
        "    CNN.add(keras.layers.Conv2D(128, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "    CNN.add(keras.layers.GlobalAveragePooling2D())\n",
        "    CNN.add(keras.layers.Dense(64, activation='relu'))\n",
        "    CNN.add(keras.layers.Dropout(0.25))\n",
        "    CNN.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    return CNN"
      ],
      "metadata": {
        "id": "Mq1w5rraWLBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as sementes ajudam a ter resultados reproduzíveis\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "random.set_seed(2)\n",
        "\n",
        "x_train_subset = x_train[:12000]\n",
        "y_train_subset = y_train[:12000]\n",
        "x_val_subset = x_test[:8000]\n",
        "y_val_subset = y_test[:8000]\n",
        "batch_size = 20\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "uhm6bFxAWMkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN1 = my_cnn()\n",
        "CNN1.compile(loss='categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.Adam(learning_rate=0.005),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "EOsgX9zBWORK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def run_experiment(x_t, y_t, x_v, y_v, batch_size, epochs):\n",
        "  \"\"\"\n",
        "  Realiza o experimento de treinamento e avaliação da rede convolucional\n",
        "\n",
        "  Args:\n",
        "    x_t: Conjunto de treinamento reduzido\n",
        "    y_t: Rótulos do conjunto de treinamento reduzido\n",
        "    x_v: Conjunto de validação\n",
        "    y_v: Rótulos do conjunto de validação\n",
        "    batch_size: Tamanho do lote\n",
        "    epochs: Número de épocas\n",
        "\n",
        "  Returns:\n",
        "    Lista com as acurácias no conjunto de treinamento e validação\n",
        "  \"\"\"\n",
        "\n",
        "  train_mean = []\n",
        "  val_mean = []\n",
        "  for _ in range(5):\n",
        "    hist = CNN1.fit(x_t, y_t,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs, validation_data=(x_v, y_v))\n",
        "    train_mean.append(np.mean(hist.history[\"accuracy\"]))\n",
        "    val_mean.append(np.mean(hist.history[\"val_accuracy\"]))\n",
        "\n",
        "  return train_mean, val_mean\n",
        "\n",
        "train_mean, val_mean = run_experiment(x_train_subset, y_train_subset, x_val_subset, y_val_subset, batch_size, epochs)\n",
        "\n",
        "print(\"Acurácia média no conjunto de treinamento:\", train_mean)\n",
        "print(\"Acurácia média no conjunto de validação:\", val_mean)"
      ],
      "metadata": {
        "id": "FmxjyO9GWPe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Acurácia média enre os conjuntos de treinamento:\", np.mean(train_mean))\n",
        "print(\"Acurácia média enre os conjuntos de validação:\", np.mean(val_mean))"
      ],
      "metadata": {
        "id": "sTSbxjZvWRxV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}